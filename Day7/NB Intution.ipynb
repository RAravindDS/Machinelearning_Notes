{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naïve Bayes\n",
    "\n",
    "- supervised learnirng algorithm \n",
    "\n",
    "\n",
    "# Assumptions made by Naïve Bayes\n",
    " \n",
    "The fundamental Naïve Bayes assumption is that each feature makes an:\n",
    "\n",
    "independent\n",
    "equal\n",
    "contribution to the outcome.\n",
    "\n",
    "For example : A furit can be considered apple if its RED,ROUND and 5cm in diameter \n",
    "\n",
    "NB classifier considers each of these features contribute independently to the probability that this fruit is apple .It dose not care about the correlation between the features\n",
    "\n",
    "\n",
    "\n",
    "# Gaussian: \n",
    "It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "\n",
    "# Multinomial: \n",
    "It is used for discrete counts. For example, let’s say,  we have a text classification problem. Here we can consider Bernoulli trials which is one step further and instead of “word occurring in the document”, we have “count how often word occurs in the document”, you can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "# Bernoulli: \n",
    "The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with ‘bag of words’ model where the 1s & 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Conditional probability\n",
    "Conditional probability is the probability of one event occurring in the presence of a second event.\n",
    "# $    p(A|B) = \\frac {p(A,B)}{p(B)}   $ #\n",
    "\n",
    "# 2.Independence\n",
    "# $    p(A|B) = {p(A)}   $ #\n",
    "\n",
    "# 3.Conditional  Independence\n",
    "when A is dependent on C and  A is not dependent on B\n",
    "# $   p(A|BC) = {p(A|C)}   $ #\n",
    " \n",
    "\n",
    "\n",
    "# 4.Joint  probability\n",
    "when something is independent then Joint Probability = Product of individual probability\n",
    "Joint probability is the probability of two events occurring simultaneously.\n",
    "\n",
    "##      p(A|B) = p(A) p(B)\n",
    "\n",
    "# $   p(A_1,A_2,..A_n|B_k) = p(A_1|B_k) P(A_2|B_k),..p(A_n|B_k)   $ \n",
    "\n",
    "# 5.Naive  Bayes \n",
    "# $   p(A|B) = \\frac {p(B|A) p(A)}{p(B)}   $ #\n",
    "We know Conditional probability = $   p(A|B) = \\frac {p(A,B)}{p(B)}   $ \n",
    "\n",
    "\n",
    "since $  \\space p(A|B) = \\frac {p(A,B)}{p(B)} $ \n",
    "\n",
    "then $  \\space p(B|A) = \\frac {p(B,A)}{p(A)} $ \n",
    "\n",
    "By Conditional probability is symmetric so by symmetric rule\n",
    "\n",
    "$ p(A|B){p(B)} = {p(A,B)}  $ \n",
    "\n",
    "$ p(A|B){p(B)} = {p(A,B)} =p(B|A){p(A)}    $ \n",
    "\n",
    "$ p(A|B){p(B)} =p(B|A){p(A)}   $ \n",
    "\n",
    "$  p(A|B) = \\frac {p(B|A) p(A)}{p(B)}   $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $   p(Y|X) = \\frac {p(X|Y) p(Y)}{p(X)}   $ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
